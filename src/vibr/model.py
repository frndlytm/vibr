from random import randint
from typing import Optional, Type

import torch
from torch import nn


class LinearSubmodule(nn.Module):
    """
    LinearSubmodule composes a Linear layer with an activation and dropout layer
    so that we can have more fine-tuned parameterized control over the layers in
    our MultiLayerPerceptron.

    Parameters
    ----------
        in_dim, units: int, int
            Input/Output dimensions, or number of units, in this particular layer

        dropout: float = 0.5
            Probability that a node will drop out of a given layer, using nn.Dropout

        activation: nn.Module = nn.ReLU
            The activation function applied. (Debating on modifying this to use
            torch.nn.functional.relu instead so we can see the difference between
            learning extra gradients on the ReLU).

    """
    def __init__(
        self,
        in_dim: int,
        units: int,
        dropout: float = 0.5,
        activation: nn.Module = nn.ReLU,
    ):
        super().__init__()
        self.linear = nn.Linear(in_dim, units)
        self.activation = activation()
        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()

        nn.init.kaiming_normal_(self.linear.weight, nonlinearity='relu')

    def forward(self, x):
        x = self.linear(x)
        x = self.activation(x)
        x = self.dropout(x)
        return x


class RepeatedSequential(nn.Sequential):
    """
    RepeatedSequential generates a Sequential submodule using a set of dimensions by
    creating multiple layers of the same moduleclass.

    Parameters
    ----------
        *dims: int
            Input/Output dimensions, or number of units, for each layer. Generates
            a sequence of layer dimensions by zipping.

        moduleclass: Type[nn.Module]
            Factory class used to generate all the layers

        **kwargs
            are forwarded to the moduleclass to configure each layer identically
            execpt for dimensions.

    """
    def __init__(self, *dims: int, moduleclass: Type[nn.Module] = LinearSubmodule, **kwargs):
        super().__init__(*[
            moduleclass(dims[i], dims[i+1], **kwargs)
            for i in range(len(dims) - 1)
        ])


class RandomizedRepeatedSequential(RepeatedSequential):
    """
    RepeatedSequential generates a Sequential submodule using a set of dimensions by
    creating multiple layers of the same moduleclass.

    Parameters
    ----------
        lower: int
            Lower bound on layer dimensions, randomly generated by random.randint

        upper: int
            Upper bound on layer dimensions, randomly generated by random.randint

        layers: int
            number of layers to generate

        **kwargs
            moduleclass and configuration kwargs to configure each layer identically
            execpt for dimensions.

    """
    def __init__(self, lower: int, upper: int, layers: int, **kwargs):
        super().__init__(self, *[randint(lower, upper) for _ in range(layers)], **kwargs)


# TODO:
#     TITLE: Layer dimension generators
#     AUTHOR: frndlytm
#     DESCRIPTION:
#
#         Write a bunch of generators for sequences of dimensions that follow
#         certain growth / decay rules. It could be really interesting to
#         evaluate various MLP dimension arrangements for multiclass classification
#
class MultiLayerPerceptron(nn.Module):
    def __init__(
        self,
        d_in: int,
        d_out: int,
        units: int,
        n_layers: int,
        dropout: float = 0.5,
        shift: Optional[torch.Tensor] = None,
        scale: Optional[torch.Tensor] = None,
    ):
        super().__init__()

        self.shift = nn.Parameter(torch.empty(d_in), requires_grad=False)
        torch.nn.init.zeros_(self.shift)
        if shift is not None:
            self.shift.data = shift

        self.scale = nn.Parameter(torch.empty(d_in), requires_grad=False)
        torch.nn.init.ones_(self.scale)
        if scale is not None:
            self.scale.data = scale

        dims = [d_in, *(units for _ in range(n_layers-1))]
        self.layers = RepeatedSequential(*dims, moduleclass=LinearSubmodule, dropout=dropout)
        self.output = torch.nn.Linear(units, d_out)
        nn.init.kaiming_normal_(self.output.weight, nonlinearity='sigmoid')

    def forward(self, x):
        x = (x - self.shift) / self.scale
        x = self.layers(x)
        return self.output(x)
