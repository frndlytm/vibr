{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from typing import Optional\n",
    "\n",
    "import tqdn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init, functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSubmodule(nn.Module):\n",
    "    \"\"\"\n",
    "    LinearSubmodule composes a Linear layer with an activation and dropout layer\n",
    "    so that we can have more fine-tuned parameterized control over the layers in\n",
    "    our MultiLayerPerceptron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        in_dim, units: int, int\n",
    "            Input/Output dimensions, or number of units, in this particular layer\n",
    "\n",
    "        dropout: float = 0.5\n",
    "            Probability that a node will drop out of a given layer, using nn.Dropout\n",
    "\n",
    "        activation: nn.Module = nn.ReLU\n",
    "            The activation function applied. (Debating on modifying this to use\n",
    "            torch.nn.functional.relu instead so we can see the difference between\n",
    "            learning extra gradients on the ReLU).\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        units: int,\n",
    "        dropout: float = 0.5,\n",
    "        activation: nn.Module = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, units)\n",
    "        self.activation = activation()\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.sequence(x)\n",
    "\n",
    "\n",
    "class RepeatedSequential(nn.Sequential):\n",
    "    \"\"\"\n",
    "    RepeatedSequential generates a Sequential submodule using a set of dimensions by\n",
    "    creating multiple layers of the same moduleclass.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        *dims: int\n",
    "            Input/Output dimensions, or number of units, for each layer. Generates\n",
    "            a sequence of layer dimensions by zipping.\n",
    "\n",
    "        moduleclass: Type[nn.Module]\n",
    "            Factory class used to generate all the layers\n",
    "\n",
    "        **kwargs\n",
    "            are forwarded to the moduleclass to configure each layer identically\n",
    "            execpt for dimensions.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, *dims: int, moduleclass: Type[nn.Module] = LinearSubmodule, **kwargs):\n",
    "        super().__init__(*[\n",
    "            moduleclass(dims[i], dims[i+1], **kwargs)\n",
    "            for i in range(len(dims) - 1)\n",
    "        ])\n",
    "\n",
    "\n",
    "class RandomizedRepeatedSequential(RepeatedSequential):\n",
    "    \"\"\"\n",
    "    RepeatedSequential generates a Sequential submodule using a set of dimensions by\n",
    "    creating multiple layers of the same moduleclass.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        lower: int\n",
    "            Lower bound on layer dimensions, randomly generated by random.randint\n",
    "\n",
    "        upper: int\n",
    "            Upper bound on layer dimensions, randomly generated by random.randint\n",
    "\n",
    "        layers: int\n",
    "            number of layers to generate\n",
    "\n",
    "        **kwargs\n",
    "            moduleclass and configuration kwargs to configure each layer identically\n",
    "            execpt for dimensions.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, lower: int, upper: int, layers: int, **kwargs):\n",
    "        super().__init__(self, *[randint(lower, upper) for _ in range(layers)], **kwargs)\n",
    "\n",
    "\n",
    "# TODO:\n",
    "#     TITLE: Layer dimension generators\n",
    "#     AUTHOR: frndlytm\n",
    "#     DESCRIPTION:\n",
    "#\n",
    "#         Write a bunch of generators for sequences of dimensions that follow\n",
    "#         certain growth / decay rules. It could be really interesting to\n",
    "#         evaluate various MLP dimension arrangements for multiclass classification\n",
    "#\n",
    "def exp_decay(start: int, stop: int, base: int) -> Iterator[int]:\n",
    "    x = start\n",
    "\n",
    "    while x >= stop:\n",
    "        yield x\n",
    "        x //= base\n",
    "\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        units: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.5,\n",
    "        shift: Optional[torch.Tensor] = None,\n",
    "        scale: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shift = nn.Parameter(torch.Tensor(in_dim), requires_grad=False)\n",
    "        torch.nn.init.zeros_(self.shift)\n",
    "        if shift is not None:\n",
    "            self.shift.data = shift\n",
    "\n",
    "        self.scale = nn.Parameter(torch.Tensor(in_dim), requires_grad=False)\n",
    "        torch.nn.init.ones_(self.scale)\n",
    "        if scale is not None:\n",
    "            self.scale.data = scale\n",
    "\n",
    "        dims = [in_dim, *(units for _ in range(n_layers-2)), out_dim]\n",
    "        self.layers = LinearSequential(*dims, moduleclass=LinearSubmodule)\n",
    "        self.output = torch.nn.Linear(n_units, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.shift) / self.scale\n",
    "        x = self.layers(x)\n",
    "        return self.output(x)\n",
    "\n",
    "    def classify(self, x, threshold: float = 0.5):\n",
    "        with torch.no_grad():\n",
    "            # TODO:\n",
    "            #     TITLE: Strategy for functional layer\n",
    "            #     AUTHOR: frndlytm\n",
    "            #     DESCRIPTION: \n",
    "            #\n",
    "            #         Decide on how we want to manage final functional layer that\n",
    "            #         actually performs the classification. This could potentially\n",
    "            #         be controlled externally.\n",
    "            #\n",
    "            y_pred = F.sigmoid(self.forward(x))\n",
    "\n",
    "        return (y_pred > threshold).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Initializer:\n",
    "    def __call__(self, model: nn.Module) -> nn.Module:\n",
    "        # TODO:\n",
    "        #     TITLE: Weight Initializer\n",
    "        #     AUTHOR: frndlytm\n",
    "        #     DESCRIPTION:\n",
    "        #\n",
    "        #         Configure layers by adding handlers that dispatch on the\n",
    "        #         type of layers using functools.singledispatch\n",
    "        #\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your PC doesn't have enough CPU Ram or Video memory, try decreasing the batch_size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "# BucketIterator allows for data to be split into buckets of equal size,\n",
    "# any remaining space is filled with pad token\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "\n",
    "# initializing model weights for better convergence\n",
    "model = MultiLayerPerceptrion(...)\n",
    "init = Initializer({...})\n",
    "init(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # optimizer to train the model\n",
    "criterion = nn.CrossEntropyLoss()                     # loss criterion\n",
    "\n",
    "# use gpu if available, These lines move your model to gpu from cpu if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# If this line prints cuda, your machine is equipped with a Nvidia GPU and\n",
    "# PyTorch is utilizing the GPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "def train(\n",
    "    epochs,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_iterator,\n",
    "    valid_iterator\n",
    "):\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = 0.0 \n",
    "        valid_loss = 0.0\n",
    "\n",
    "        # start training\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            text, tags = batch.text, batch.udtags\n",
    "\n",
    "            # zero the gradients from last batch\n",
    "            # feed the batch to the model\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(text)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            # Evaluate the loss and propagate during training.\n",
    "            loss = criterion(predictions, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Cache the stats for epoch logging\n",
    "            train_acc = categorical_accuracy(predictions, tags)\n",
    "            train_loss += loss.data.item() * text.size(0)\n",
    "\n",
    "        train_loss /= len(train_iterator)\n",
    "\n",
    "        # start validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(valid_iterator):\n",
    "                text, tags = batch.text, batch.udtags\n",
    "\n",
    "                predictions = model(text)\n",
    "                predictions = predictions.view(-1, predictions.shape[-1])\n",
    "                tags = tags.view(-1)\n",
    "                loss = criterion(predictions, tags)\n",
    "\n",
    "                valid_acc = categorical_accuracy(predictions, tags)\n",
    "                valid_loss += loss.data.item() * text.size(0)\n",
    "\n",
    "        valid_loss /= len(valid_iterator)\n",
    "\n",
    "        # log stats\n",
    "        logging.info('\\n'.join([\n",
    "            f'Epoch: {epoch+1:02}',\n",
    "            f'Training Loss: {train_loss:.2f}',\n",
    "            f'Training Accuracy: {train_acc:.2f}',\n",
    "            f'Validation Loss: {valid_loss:.2f}',\n",
    "            f'Validation Accuracy: {valid_acc:.2f}',\n",
    "            '\\n'\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    EPOCHS,\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_iterator,\n",
    "    valid_iterator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the accuracy on test set\n",
    "def test(model, test_iterator):\n",
    "    test_acc=0\n",
    "\n",
    "    # Computes without the gradients. Use this while testing your model.\n",
    "    # As we do not intend to learn from the data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iterator:\n",
    "            predictions = model(text)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "    test_acc /= len(test_iterator)\n",
    "\n",
    "    logging.info(f'Test Acc: {test_acc:.2f}\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
