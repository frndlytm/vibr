{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from random import randint\n",
    "from typing import Optional, Type\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init, functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSubmodule(nn.Module):\n",
    "    \"\"\"\n",
    "    LinearSubmodule composes a Linear layer with an activation and dropout layer\n",
    "    so that we can have more fine-tuned parameterized control over the layers in\n",
    "    our MultiLayerPerceptron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        in_dim, units: int, int\n",
    "            Input/Output dimensions, or number of units, in this particular layer\n",
    "\n",
    "        dropout: float = 0.5\n",
    "            Probability that a node will drop out of a given layer, using nn.Dropout\n",
    "\n",
    "        activation: nn.Module = nn.ReLU\n",
    "            The activation function applied. (Debating on modifying this to use\n",
    "            torch.nn.functional.relu instead so we can see the difference between\n",
    "            learning extra gradients on the ReLU).\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        units: int,\n",
    "        dropout: float = 0.5,\n",
    "        activation: nn.Module = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, units)\n",
    "        self.activation = activation()\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RepeatedSequential(nn.Sequential):\n",
    "    \"\"\"\n",
    "    RepeatedSequential generates a Sequential submodule using a set of dimensions by\n",
    "    creating multiple layers of the same moduleclass.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        *dims: int\n",
    "            Input/Output dimensions, or number of units, for each layer. Generates\n",
    "            a sequence of layer dimensions by zipping.\n",
    "\n",
    "        moduleclass: Type[nn.Module]\n",
    "            Factory class used to generate all the layers\n",
    "\n",
    "        **kwargs\n",
    "            are forwarded to the moduleclass to configure each layer identically\n",
    "            execpt for dimensions.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, *dims: int, moduleclass: Type[nn.Module] = LinearSubmodule, **kwargs):\n",
    "        super().__init__(*[\n",
    "            moduleclass(dims[i], dims[i+1], **kwargs)\n",
    "            for i in range(len(dims) - 1)\n",
    "        ])\n",
    "\n",
    "\n",
    "class RandomizedRepeatedSequential(RepeatedSequential):\n",
    "    \"\"\"\n",
    "    RepeatedSequential generates a Sequential submodule using a set of dimensions by\n",
    "    creating multiple layers of the same moduleclass.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        lower: int\n",
    "            Lower bound on layer dimensions, randomly generated by random.randint\n",
    "\n",
    "        upper: int\n",
    "            Upper bound on layer dimensions, randomly generated by random.randint\n",
    "\n",
    "        layers: int\n",
    "            number of layers to generate\n",
    "\n",
    "        **kwargs\n",
    "            moduleclass and configuration kwargs to configure each layer identically\n",
    "            execpt for dimensions.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, lower: int, upper: int, layers: int, **kwargs):\n",
    "        super().__init__(self, *[randint(lower, upper) for _ in range(layers)], **kwargs)\n",
    "\n",
    "\n",
    "# TODO:\n",
    "#     TITLE: Layer dimension generators\n",
    "#     AUTHOR: frndlytm\n",
    "#     DESCRIPTION:\n",
    "#\n",
    "#         Write a bunch of generators for sequences of dimensions that follow\n",
    "#         certain growth / decay rules. It could be really interesting to\n",
    "#         evaluate various MLP dimension arrangements for multiclass classification\n",
    "#\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        units: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.5,\n",
    "        shift: Optional[torch.Tensor] = None,\n",
    "        scale: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shift = nn.Parameter(torch.empty(d_in), requires_grad=False)\n",
    "        torch.nn.init.zeros_(self.shift)\n",
    "        if shift is not None:\n",
    "            self.shift.data = shift\n",
    "\n",
    "        self.scale = nn.Parameter(torch.empty(d_in), requires_grad=False)\n",
    "        torch.nn.init.ones_(self.scale)\n",
    "        if scale is not None:\n",
    "            self.scale.data = scale\n",
    "\n",
    "        dims = [d_in, *(units for _ in range(n_layers-1))]\n",
    "        self.layers = RepeatedSequential(*dims, moduleclass=LinearSubmodule, dropout=dropout)\n",
    "        self.output = torch.nn.Linear(units, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape, self.shift.shape, self.scale.shape)\n",
    "        x = (x - self.shift) / self.scale\n",
    "        x = self.layers(x)\n",
    "        return self.output(x)\n",
    "\n",
    "    def classify(self, x, threshold: float = 0.5):\n",
    "        with torch.no_grad():\n",
    "            # TODO:\n",
    "            #     TITLE: Strategy for functional layer\n",
    "            #     AUTHOR: frndlytm\n",
    "            #     DESCRIPTION:\n",
    "            #\n",
    "            #         Decide on how we want to manage final functional layer that\n",
    "            #         actually performs the classification. This could potentially\n",
    "            #         be controlled externally.\n",
    "            #\n",
    "            y_pred = F.sigmoid(self.forward(x))\n",
    "\n",
    "        return (y_pred > threshold).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import random\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logging.getLogger('notebook').setLevel(logging.DEBUG)\n",
    "\n",
    "# Using a seed to maintain consistent and reproducible results\n",
    "SEED = 100\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your PC doesn't have enough CPU Ram or Video memory, try decreasing the batch_size\n",
    "BATCH_SIZE = 128\n",
    "DATA_DIR = path.join(\n",
    "    path.dirname(path.dirname(path.realpath(\"__file__\"))),  \"data\"\n",
    ")\n",
    "\n",
    "def load_data(batch_size: int = BATCH_SIZE, datadir: str = DATA_DIR):\n",
    "    # Grab the feature file and moods targets from listening moods data\n",
    "    features = np.load(path.join(datadir, 'tp_source_trimmed.npy'), allow_pickle=True)\n",
    "    moods = np.load(path.join(datadir, f'moods_target_trimmed.npy'), allow_pickle=True)\n",
    "    moods = np.stack(moods[:, 1]).astype(float)\n",
    "\n",
    "    # TODO:\n",
    "    #     TITLE: Uncomment to use shared splits\n",
    "    #     AUTHOR: frndlytm\n",
    "    #\n",
    "    # # Grab the cached indexes from the listening-moods training\n",
    "    # train_idxs, val_idxs, test_idxs = (\n",
    "    #     np.load(path.join(datadir, f'train_idx.npy')),\n",
    "    #     np.load(path.join(datadir, f'val_idx.npy')),\n",
    "    #     np.load(path.join(datadir, f'test_idx.npy')),\n",
    "    # )\n",
    "    size = int(features.shape[1])\n",
    "    train_size, valid_size, test_size = (\n",
    "        int(0.6 * size), int(0.2 * size), int(0.2 * size),\n",
    "    )\n",
    "    train_idxs, val_idxs, test_idxs = (\n",
    "        slice(0, train_size, 1),\n",
    "        slice(train_size, train_size+valid_size, 1),\n",
    "        slice(train_size+valid_size, -1, 1),\n",
    "    )\n",
    "\n",
    "    # Construct TensorDatasets for each index\n",
    "    train_data, val_data, test_data = (\n",
    "        TensorDataset(\n",
    "            torch.from_numpy(features[train_idxs]),\n",
    "            torch.from_numpy(moods[train_idxs])\n",
    "        ),\n",
    "        TensorDataset(\n",
    "            torch.from_numpy(features[val_idxs]),\n",
    "            torch.from_numpy(moods[val_idxs])\n",
    "        ),\n",
    "        TensorDataset(\n",
    "            torch.from_numpy(features[test_idxs]),\n",
    "            torch.from_numpy(moods[test_idxs])\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # RETURN DataLoader batch iterators on the data\n",
    "    return {\n",
    "        \"d_in\": features.shape[1],\n",
    "        \"d_out\": moods.shape[1],\n",
    "        \"datasets\":{\n",
    "            \"train\": DataLoader(train_data, batch_size=batch_size),\n",
    "            \"valid\": DataLoader(val_data, batch_size=batch_size),\n",
    "            \"test\": DataLoader(test_data, batch_size=batch_size),\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Load and unpack the data\n",
    "d_in, d_out, datasets = load_data(batch_size=BATCH_SIZE).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# initializing model weights for better convergence\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MultiLayerPerceptron(d_in=d_in, d_out=d_out, units=2, n_layers=2)\n",
    "\n",
    "# init(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # optimizer to train the model\n",
    "criterion = nn.CrossEntropyLoss()                     # loss criterion\n",
    "\n",
    "# use gpu if available, These lines move your model to gpu from cpu if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# If this line prints cuda, your machine is equipped with a Nvidia GPU and\n",
    "# PyTorch is utilizing the GPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_iterator,\n",
    "    valid_iterator,\n",
    "    epochs: int = EPOCHS,\n",
    "    device: str = \"cpu\"\n",
    "):\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = 0.0 \n",
    "        valid_loss = 0.0\n",
    "\n",
    "        # start training\n",
    "        model.train()\n",
    "        for (inputs, targets) in train_iterator:\n",
    "            # zero the gradients from last batch\n",
    "            # feed the batch to the model\n",
    "            optimizer.zero_grad()\n",
    "            inputs.to(device)\n",
    "            targets.to(device)\n",
    "\n",
    "            # Evaluate the loss and propagate during training.\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Cache the stats for epoch logging\n",
    "            train_loss += loss.data.item()\n",
    "\n",
    "        train_loss /= len(train_iterator)\n",
    "\n",
    "        # start validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets) in valid_iterator:\n",
    "                # zero the gradients from last batch\n",
    "                # feed the batch to the model\n",
    "                optimizer.zero_grad()\n",
    "                inputs.to(device)\n",
    "                targets.to(device)\n",
    "\n",
    "                # Evaluate the loss and propagate during training.\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                valid_loss += loss.data.item()\n",
    "\n",
    "        valid_loss /= len(valid_iterator)\n",
    "\n",
    "        # log stats\n",
    "        print(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"Epoch\": f\"{epoch+1:02}\",\n",
    "                    \"Training Loss\": f\"{train_loss:.2f}\",\n",
    "                    \"Validation Loss\": f\"{valid_loss:.2f}\",\n",
    "                }\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 49.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 200]) torch.Size([200]) torch.Size([200])\n",
      "torch.Size([40, 200]) torch.Size([200]) torch.Size([200])\n",
      "{\"Epoch\": \"01\", \"Training Loss\": \"69.11\", \"Validation Loss\": \"66.56\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    datasets[\"train\"],\n",
    "    datasets[\"valid\"],\n",
    "    epochs=1,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the accuracy on test set\n",
    "def test(model, test_iterator):\n",
    "    test_acc=0\n",
    "\n",
    "    # Computes without the gradients. Use this while testing your model.\n",
    "    # As we do not intend to learn from the data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (inputs, targets) in test_iterator:\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "    test_acc /= len(test_iterator)\n",
    "\n",
    "    logging.info(f'Test Acc: {test_acc:.2f}\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50e9ea9da83c85c1672c778af98a03119a102915440355947823f3b790968db2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('vibr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
