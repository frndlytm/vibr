{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SVM Fitting and Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn import preprocessing\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define Class for running bagged SVM on a Multilabel class set:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiLabelSVM:\n",
    "\n",
    "    def __init__(self, B):\n",
    "        '''\n",
    "        Initialize a class instance.\n",
    "        :param B: Number of bagged samples to run\n",
    "        '''\n",
    "        self.fit_models = None\n",
    "        self.B = B\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Fit the features to given labels using bagged SVM method.\n",
    "        :param X: features vectors for each example\n",
    "        :param y: multilabel vectors for each example\n",
    "        :return:\n",
    "        '''\n",
    "        classes = len(y[0])\n",
    "        self.fit_models = []\n",
    "\n",
    "        #work through each class in the multilabel\n",
    "        for i in range(classes):\n",
    "\n",
    "            #combine features and labels so we can split up\n",
    "            #again for bagging\n",
    "            X = pd.DataFrame(X)\n",
    "            X['target'] = y[:,i]\n",
    "\n",
    "            #split up into examples with negative labels and positive labels\n",
    "            X_pos = X[X['target'] == 1]\n",
    "            X_neg = X[X['target'] == 0]\n",
    "\n",
    "            #store all bagged models\n",
    "            models = []\n",
    "\n",
    "            for n in range(self.B):\n",
    "\n",
    "                #Bagging:\n",
    "                #get the number of positive labels\n",
    "                num_pos = len(X_pos)\n",
    "\n",
    "                #create bootstrapped samples of positive example size\n",
    "                p = X_pos.sample(n=num_pos, replace=True)\n",
    "                neg = X_neg.sample(n=num_pos, replace=True)\n",
    "\n",
    "                #re-combine the positive and negative examples\n",
    "                data = pd.concat([p, neg])\n",
    "\n",
    "                #Fit and train the SVC\n",
    "                m_i = SVC()\n",
    "                m_i.fit(data.drop(columns='target'), data['target'])\n",
    "\n",
    "                #add to tracked models\n",
    "                models.append(m_i)\n",
    "\n",
    "            self.fit_models.append(models)\n",
    "\n",
    "\n",
    "    def feature_accuracy(self, x_ex, y_known):\n",
    "        '''\n",
    "        Custom defined accuracy measure to track accuracy of each individual label.\n",
    "        :param x_ex: Given examples to predict.\n",
    "        :param y_known: Known multilabels for each example.\n",
    "        :return: dictionary for each label class and their metrics.\n",
    "        '''\n",
    "        out = {}\n",
    "\n",
    "        for f in range(len(self.fit_models)):\n",
    "            true = y_known[:, f]\n",
    "\n",
    "            avg_acc, avg_rec, avg_prec = 0,0,0\n",
    "\n",
    "            #average over all bootsrapped models\n",
    "            for m in range(len(self.fit_models[f])):\n",
    "\n",
    "                pred = self.fit_models[f][m].predict(x_ex)\n",
    "                acc = accuracy_score(true, pred)\n",
    "                rec = recall_score(true, pred)\n",
    "                prec = precision_score(true, pred)\n",
    "\n",
    "                avg_acc += acc\n",
    "                avg_rec += rec\n",
    "                avg_prec += prec\n",
    "\n",
    "            avg_acc = avg_acc / len(self.fit_models[f])\n",
    "            avg_rec = avg_rec / len(self.fit_models[f])\n",
    "            avg_prec = avg_prec / len(self.fit_models[f])\n",
    "            out[f] = (avg_acc, avg_rec, avg_prec)\n",
    "\n",
    "        print(\"results \", out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load and preprocess features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = np.load(\"../data/tp_source_trimmed.npy\", allow_pickle=True)\n",
    "moods = np.load(\"../data/moods_target_trimmed.npy\", allow_pickle=True)[:, 1]\n",
    "m_len = len(moods[0])\n",
    "\n",
    "features = preprocessing.normalize(features)\n",
    "\n",
    "for i in range(len(moods)):\n",
    "    moods[i] = np.array(moods[i])\n",
    "    moods = np.stack(moods)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create test train split for trial."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, moods, test_size=0.2, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train features with bagged multi label SVM."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "multi = MultiLabelSVM(B=30)\n",
    "multi.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get results output for trained model and save to file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats = multi.feature_accuracy(X_test, y_test)\n",
    "\n",
    "with open(\"../results/svm_bagged_results.json\", \"w\") as outfile:\n",
    "    json.dump(stats, outfile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Part 2 - Results analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load in saved results file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../results/svm_bagged_results.json\", \"r\") as json_file:\n",
    "    results = json.load(json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Process file and collect results sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "recall = []\n",
    "precision = []\n",
    "\n",
    "for i in results.keys():\n",
    "    accuracy.append(results[i][0])\n",
    "    precision.append(results[i][1])\n",
    "    recall.append(results[i][2])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get mood labels for results display."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = \"../data/moods.txt\"\n",
    "l = []\n",
    "\n",
    "f = open(labels, \"r\")\n",
    "for x in f:\n",
    "    l.append(str.strip(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display results as average, max and min for accuracy, precision, and recall."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(max(accuracy), l[np.argmin(accuracy)])\n",
    "print(min(accuracy), l[np.argmax(accuracy)])\n",
    "print(np.mean(accuracy), '\\n')\n",
    "\n",
    "print(max(precision), l[np.argmin(precision)])\n",
    "print(min(precision), l[np.argmax(precision)])\n",
    "print(np.mean(precision), '\\n')\n",
    "\n",
    "print(max(recall), l[np.argmin(recall)])\n",
    "print(min(recall), l[np.argmax(recall)])\n",
    "print(np.mean(recall), '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create counts for each mood -- measure the number of positive examples for\n",
    "each label out of all labels for that mood."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mood_counts = []\n",
    "\n",
    "for i in range(len(l)):\n",
    "    col = moods[:, i]\n",
    "    ones = np.bincount(col)[1]\n",
    "    mood_counts.append(ones)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a plot to compare percentage of positive labels versus recall\n",
    "performance of model on that mood."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m = np.array(mood_counts)/len(mood_counts)\n",
    "x = accuracy\n",
    "\n",
    "plt.scatter(x, m)\n",
    "\n",
    "plt.xlabel(\"Recall Performance of SVM Model\")\n",
    "plt.ylabel(\"Percentage of Positive Examples in Each Mood\")\n",
    "plt.title(\"Positive Class Frequency Versus Recall Score\")\n",
    "\n",
    "# This will fit the best line into the graph\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, m, 1))\n",
    "(np.unique(x)), color='red')\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}